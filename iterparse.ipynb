{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import iterparse\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"lang_list.tsv\", sep=\"\\t\")\n",
    "lang_dict = {k:v for k, v in zip(df[\"Language Code\"], df[\"Language Name (Italian)\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dict = {}\n",
    "lang_pattern = re.compile(\"=={{-?(.+?)-?}}==\")\n",
    "pos_pattern = re.compile(\"{{-(.*?)-\\|(?:\\|?\\w*)*}}\")\n",
    "morpho_pattern = re.compile(\"{{Pn.*?}}(?:\\s{1,3})?''(.*?)''\") # a volte ci sono più (o non ci sono) spazi prima della morpho\n",
    "glossa_pattern = re.compile(\"\\[\\[(\\w*?(?:\\s\\w*?)?)\\]\\]|\\[\\[\\w*?(?:#\\w*)?\\|(.*?)\\]\\]\")\n",
    "special_redirect_pattern = re.compile(\"\\[\\[:?\\w+:\\w*\\s?\\w*\\|(\\w*\\s?\\w*)\\]\\]\") # [[:w:.... ... | .... ....]] \n",
    "quote_marks_pattern = re.compile(\"'{2,3}\") # rimuove le virgolette se doppie o triple, notazione di wikipedia per il reindirizzamento\n",
    "ipa_pattern = re.compile(\"{{IPA\\|\\/(.*?)\\/}}\")\n",
    "sill_pattern = re.compile(\"{{-sill-}}\")\n",
    "etim_pattern = re.compile(\"{{-etim-}}\")\n",
    "pron_pattern = re.compile(\"{{-pron-}}\")\n",
    "frequent_pos = ['verb form', 'sost', 'sost form', 'agg form', 'agg', 'verb', 'nome', 'avv', 'loc nom', 'acron', 'agg num', 'espr', 'pref', 'card', 'suff', 'inter', 'cong', 'prep', 'pronome']\n",
    "lang_pointer_pattern = re.compile(\"{{(\\w+)}}\")\n",
    "all_lemmas = []\n",
    "\n",
    "\n",
    "def prepend_ns(s):\n",
    "    return '{http://www.mediawiki.org/xml/export-0.10/}' + s # automate the ns find\n",
    "\n",
    "def lang_check(line, lemma):\n",
    "    line = line.replace(\" \", \"\") # eliminiamo gli spazi per conformità\n",
    "    match =  re.search(lang_pattern, line) # lingua\n",
    "    if match != None:\n",
    "        lang = match.group(1)\n",
    "        if lang == None:\n",
    "            lang = match.group(2)\n",
    "        if lang == \"it\":\n",
    "            parsed_dict[lemma] = {\"meta\": {\"ipa\": [], \"sill\": [], \"etim\": \"\"}, \"meanings\": {}}\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def pron_check(line):\n",
    "    match = re.search(pron_pattern, line)\n",
    "    if match != None:\n",
    "        return True\n",
    "\n",
    "def get_ipa(line, lemma):\n",
    "    match = re.search(ipa_pattern, line) # ipa\n",
    "    if match != None:\n",
    "        ipa = match.group(1)\n",
    "        parsed_dict[lemma][\"meta\"][\"ipa\"].append(ipa)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def sill_check(line):\n",
    "    \"\"\"To check if the sill token is present in the line\"\"\"\n",
    "    match = re.search(sill_pattern, line) # sill\n",
    "    if match != None:\n",
    "        return True\n",
    "    \n",
    "def get_sill(line, lemma):\n",
    "    if line[0] == \";\":\n",
    "        parsed_dict[lemma][\"meta\"][\"sill\"] = line[1:].replace(\" \", \"\").split(\"|\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def pos_check(line):\n",
    "    match = re.search(pos_pattern, line) # pos\n",
    "    if match != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def unk_pos(lemma):\n",
    "    pos = \"unk\" # opzione di default (poichè il formato non è consistente)\n",
    "    parsed_dict[lemma][\"meanings\"][pos] = {\"morpho\":\"\", \"glossa\":\"\"}\n",
    "    return pos\n",
    "\n",
    "def check_pos(lemma, line, i_pos, current_pos):\n",
    "    match = re.search(pos_pattern, line) # pos\n",
    "    if match != None:\n",
    "        pos = match.group(1) + f\"_{i_pos}\"\n",
    "        if pos != current_pos and pos != f\"Varie lingue_{i_pos}\":\n",
    "            current_pos = pos\n",
    "            parsed_dict[lemma][\"meanings\"][pos] = {\"morpho\":\"\", \"glossa\":\"\"}\n",
    "            if \"unk\" in parsed_dict[lemma][\"meanings\"]:\n",
    "                del parsed_dict[lemma][\"meanings\"][\"unk\"] # se dopo aver messo unk si trova una pos si elimina unk\n",
    "        return pos\n",
    "    else:\n",
    "        return current_pos\n",
    "    \n",
    "def morpho_check(line, lemma, pos):\n",
    "    match = re.search(morpho_pattern, line) # informazioni morfologiche\n",
    "    if match != None:\n",
    "        morpho = match.group(1).lstrip() # talvolta c'è uno spazio all'inizio\n",
    "        parsed_dict[lemma][\"meanings\"][pos][\"morpho\"] = morpho\n",
    "        return True\n",
    "\n",
    "def etim_check(line):\n",
    "    match = re.search(etim_pattern, line)\n",
    "    if match != None:\n",
    "        return True\n",
    "\n",
    "def get_etim(line, lemma):\n",
    "    cleaned_line = re.sub(\"{{Pn}}\", lemma, line)\n",
    "    cleaned_line = re.sub(lang_pointer_pattern, lambda m: lang_dict.get(m.group(1), m.group(0)), cleaned_line)\n",
    "    cleaned_line = re.sub(special_redirect_pattern, r\"\\1\", cleaned_line)\n",
    "    cleaned_line = re.sub(\"{{.*?}}\", \"\", cleaned_line)\n",
    "    cleaned_line = re.sub(glossa_pattern, lambda m: m.group(1) if m.group(2) is None else m.group(2), cleaned_line) # rimuove [[...]] o [[...|...]] tenendo la prima parola\n",
    "    cleaned_line = re.sub(\"\\[\\[\\w.*?\\]\\]\", \"\", cleaned_line)\n",
    "    cleaned_line = re.sub(quote_marks_pattern, \"\", cleaned_line)\n",
    "    cleaned_line = cleaned_line.lstrip()\n",
    "    if parsed_dict[lemma][\"meta\"][\"etim\"] == \"\":\n",
    "        parsed_dict[lemma][\"meta\"][\"etim\"] += cleaned_line\n",
    "    else:\n",
    "        parsed_dict[lemma][\"meta\"][\"etim\"] += \"\\n\"+cleaned_line\n",
    "    \n",
    "def glossa_check(line, lemma, pos):\n",
    "    line = line[1:]\n",
    "    cleaned_line = re.sub(\"{{Pn}}\", lemma, line)\n",
    "    cleaned_line = re.sub(lang_pointer_pattern, lambda m: lang_dict.get(m.group(1), m.group(0)), cleaned_line)\n",
    "    cleaned_line = re.sub(special_redirect_pattern, r\"\\1\", cleaned_line)\n",
    "    cleaned_line = re.sub(\"{{.*?}}\", \"\", cleaned_line)\n",
    "    cleaned_line = re.sub(glossa_pattern, lambda m: m.group(1) if m.group(2) is None else m.group(2), cleaned_line) # rimuove [[...]] o [[...|...]] tenendo la prima parola\n",
    "    cleaned_line = re.sub(\"\\[\\[\\w.*?\\]\\]\", \"\", cleaned_line)\n",
    "    cleaned_line = re.sub(quote_marks_pattern, \"\", cleaned_line)\n",
    "    cleaned_line = cleaned_line.lstrip()\n",
    "    if parsed_dict[lemma][\"meanings\"][pos][\"glossa\"] == \"\":\n",
    "        parsed_dict[lemma][\"meanings\"][pos][\"glossa\"] += cleaned_line\n",
    "    else:\n",
    "        parsed_dict[lemma][\"meanings\"][pos][\"glossa\"] += \"\\n\"+cleaned_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    context = iterparse(\"itwiktionary-20230620-pages-articles.xml\", events=(\"start\", \"end\"))\n",
    "\n",
    "    for event, elem in tqdm(context, desc=\"Parsing XML\", unit=\" elements\"):\n",
    "        if event == \"end\":\n",
    "            if prepend_ns(\"page\") == elem.tag:\n",
    "                lemma = elem.find(prepend_ns(\"title\"))\n",
    "                if lemma != None:\n",
    "                    lemma = str(lemma.text)\n",
    "                    if \":\" in lemma or lemma in [\"Pagina principale\", \"Pagina principale/Categorie\"]:\n",
    "                        continue\n",
    "                    all_lemmas.append(lemma)\n",
    "                    revision = elem.find(prepend_ns(\"revision\"))\n",
    "                    glossa = revision.find(prepend_ns(\"text\")).text\n",
    "                    current_pos = \"\"\n",
    "                    lang_found = False\n",
    "                    sill_flag = False\n",
    "                    unk_pos_flag = False\n",
    "                    elenco_flag = False\n",
    "                    etim_flag = False\n",
    "                    pron_flag = False\n",
    "                    i_pos = 0\n",
    "\n",
    "                    try:\n",
    "                        lines = glossa.splitlines()\n",
    "                        for i in range(len(lines)):\n",
    "                            line = lines[i]\n",
    "\n",
    "                            if line == \"\":\n",
    "                                etim_flag = False\n",
    "                                pron_flag = False\n",
    "                                sill_flag = False\n",
    "                                continue\n",
    "\n",
    "                            if pron_flag:\n",
    "                                pron_flag = get_ipa(line, lemma)\n",
    "                                if pron_flag:\n",
    "                                    continue # di ipa possono essercene più di una\n",
    "\n",
    "                            if sill_flag:\n",
    "                                found = get_sill(line, lemma)\n",
    "                                if found:\n",
    "                                    sill_flag = False\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    sill_flag = False\n",
    "                            \n",
    "                            if etim_flag:\n",
    "                                if line[0] == \"#\" or line[0] == \"*\" or line[0] == \":\":\n",
    "                                    get_etim(line[1:], lemma)\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    get_etim(line, lemma)\n",
    "                                    etim_flag = False                              \n",
    "                                    continue \n",
    "\n",
    "                            if line.find(\"{{Vedi|\") != -1:\n",
    "                                continue\n",
    "                            if line[0] == \"[\": #immagini\n",
    "                                continue\n",
    "\n",
    "                            if line[0] == \"=\":\n",
    "                                lang_found = lang_check(line, lemma)\n",
    "                            if not lang_found:\n",
    "                                continue\n",
    "                            \n",
    "                            if not unk_pos_flag: # di default si mette sempre una pos unk\n",
    "                                current_pos = unk_pos(lemma)\n",
    "                                unk_pos_flag = True\n",
    "                            \n",
    "                            pos = check_pos(lemma, line, i_pos, current_pos)\n",
    "                            if current_pos != pos:\n",
    "                                etim_flag = False\n",
    "                                i_pos +=1\n",
    "                                current_pos = pos\n",
    "\n",
    "                            pron_flag = pron_check(line)\n",
    "                            if pron_flag:\n",
    "                                continue\n",
    "                            \n",
    "                            sill_flag = sill_check(line)\n",
    "                            if sill_flag:\n",
    "                                continue # se trovo il token della sill allora passo alla iter successiva\n",
    "                            \n",
    "                            if morpho_check(line, lemma, current_pos):\n",
    "                                continue\n",
    "\n",
    "                            etim_flag = etim_check(line)\n",
    "                            if etim_flag:\n",
    "                                continue\n",
    "\n",
    "                            if line[0] == \"#\": # glossa\n",
    "                                if line[-1] == \":\": # introduce elenco\n",
    "                                    elenco_flag = True\n",
    "                                try:\n",
    "                                    if line[1] == \"*\" and not elenco_flag: # esempio\n",
    "                                        continue\n",
    "                                except IndexError:\n",
    "                                    continue # linea[1:] è vuota\n",
    "                                glossa_check(line, lemma, current_pos)\n",
    "                            else:\n",
    "                                elenco_flag = False # se dopo un elenco puntato ho una lista di esempi?\n",
    "                                                \n",
    "                    except Exception as e:\n",
    "                        print(\"ERRORE al lemma\", lemma)\n",
    "                        raise\n",
    "\n",
    "    del context\n",
    "\n",
    "# casa etim\n",
    "# albicocca etim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing XML: 19300866 elements [02:03, 156032.88 elements/s]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369773, 562905, 193132)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found_lemmas = [x for x in all_lemmas if x not in parsed_dict]\n",
    "len(parsed_dict), len(all_lemmas), len(not_found_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'ipa': ['siɡaˈretta'], 'sill': ['si', 'ga', 'rét', 'ta'], 'etim': 'diminutivo di sigaro, similmente al francese cigarette'}, 'meanings': {'sost_0': {'morpho': 'f sing', 'glossa': 'rotolino di forma cilindrica contenente tabacco trinciato avvolto in una cartina destinato ad essere fumato\\nqualsiasi oggetto avente forma simile a quella di una sigaretta\\npiccola bobina di cartoncino attorno a cui è avvolto filo per cucire'}}}\n"
     ]
    }
   ],
   "source": [
    "print(parsed_dict[\"sigaretta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3895"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown = []\n",
    "for k in parsed_dict:\n",
    "    one_def = False\n",
    "    try:\n",
    "        for m in parsed_dict[k][\"meanings\"]:\n",
    "            if parsed_dict[k][\"meanings\"][m][\"glossa\"] != \"\":\n",
    "                one_def = True\n",
    "            \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    if one_def == False:\n",
    "        unknown.append(k)\n",
    "\n",
    "len(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"it-dictionary.json\", \"w\") as f:\n",
    "    json.dump(parsed_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
